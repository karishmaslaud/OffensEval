{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8744, 3)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas\n",
    "'''\n",
    "with open(\"Greek/offenseval-greek-training-v1.tsv\") as fd:\n",
    "    rd = csv.reader(fd, delimiter=\"\\t\", quotechar='\"')\n",
    "    for row in rd:\n",
    "        print(row)\n",
    " '''       \n",
    "headers=['id','tweet','subtask_a']\n",
    "data = pandas.read_csv(\"Greek/offenseval-greek-training-v1.tsv\", delimiter='\\t',names=headers)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id                                              tweet subtask_a\n",
      "1  1172  @USER Οι μουσουλμάνες που τις βιάζουν έτσι κ α...       OFF\n",
      "2  4078  Η Κάτια προσπαθεί να πείσει οτι δεν είναι ελέφ...       NOT\n",
      "3   135  Καλά γιατί λες ότι, είσαι νέος αφού γεννήθηκες...       NOT\n",
      "4  9056          Με Φατσεα ξεκινησαμε...... #Kokkinopotami       NOT\n",
      "5  5344                  #gntmgr Κάτια πόσο γλυκιά, εμετός       NOT\n"
     ]
    }
   ],
   "source": [
    "data=data[1:]\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8743, 3)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#https://towardsdatascience.com/linear-regression-in-6-lines-of-python-5e1d0cd05b8d\n",
    "#https://stackoverflow.com/questions/13187778/convert-pandas-dataframe-to-numpy-array\n",
    "dfnumpy=data.to_numpy();\n",
    "x=dfnumpy[:, 1].reshape(-1, 1)\n",
    "y=dfnumpy[:, 2].reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8743, 1)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.33, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5857, 1)\n",
      "(8743, 1)\n",
      "(5857, 1)\n",
      "(2886, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(x.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tokenizer = Tokenizer(nlp.vocab)\\nprint(nlp)\\n#print(tokenizer)\\nfor txt in x:\\n    tokens = tokenizer(text[0])\\n    print(tokens)\\n    mydoc=nlp(txt[0])\\n    print([t1.text for t1 in mydoc])\\n    \\nfor text in x:\\n   print(text[0].split(\" \"))\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''tokenizer = Tokenizer(nlp.vocab)\n",
    "print(nlp)\n",
    "#print(tokenizer)\n",
    "for txt in x:\n",
    "    tokens = tokenizer(text[0])\n",
    "    print(tokens)\n",
    "    mydoc=nlp(txt[0])\n",
    "    print([t1.text for t1 in mydoc])\n",
    "    \n",
    "for text in x:\n",
    "   print(text[0].split(\" \"))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrt=x[:,0]\n",
    "#TOOD\n",
    "#arrt=[lamda m: for m in arrt re.sub(r' #', 'HASHTAG', arrt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8743,)\n"
     ]
    }
   ],
   "source": [
    "print(arrt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "allTokens=[];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "\n",
    "#LEMMATIZATION\n",
    "import spacy\n",
    "import el_core_news_sm \n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "nlp=el_core_news_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-93df5f904e73>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-93df5f904e73>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    allTokens.append(x2)\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def preprocess(arrt):\n",
    "    allTokens=[]\n",
    "    noises = ['URL', '@USER', '\\'ve', 'n\\'t', '\\'s', '\\'m',\"’\"]\n",
    "    stopwords =[]\n",
    "    for txt in arrt:\n",
    "        x1=tknzr.tokenize(txt)\n",
    "        x2=list(filter(lambda t: t not in string.punctuation and t not in noises ,x1)\n",
    "        #print(x1)\n",
    "        #if len(x2)!=0:dont remove 0 list as issue in svm training\n",
    "        allTokens.append(x2)\n",
    "    return allTokens\n",
    "\n",
    "allTokens=preprocess(arrt)\n",
    "print(arrt.shape)\n",
    "print(allTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(arrt):\n",
    "    noises = ['tweet','URL', '@USER', '\\'ve', 'n\\'t', '\\'s', '\\'m',\"’\"]\n",
    "    allTokens =[]\n",
    "    for txt in arrt:\n",
    "        x1=tknzr.tokenize(txt)\n",
    "        x2=[]\n",
    "        for t in x1:\n",
    "            if t not in string.punctuation and t not in noises:\n",
    "                x2.append(t)\n",
    "                #x2=list(filter(lambda t: t not in string.punctuation and t not in noises ,x1)\n",
    "                \n",
    "        #if len(x2)!=0 :removed as y needs x\n",
    "        allTokens.append(list(x2))\n",
    "               \n",
    "    return allTokens\n",
    "\n",
    "allTokens=preprocess(arrt)\n",
    "print(arrt.shape)\n",
    "print(len(allTokens))\n",
    "\n",
    "print(len(allTokens[0:8744]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOKENIZATION USING SPACY \n",
    "# BUT NOT TWEET TOEKNIZED\n",
    "# too time consuming\n",
    "'''\n",
    "allTokens1=[]\n",
    "for token in x:\n",
    "    x2=[]\n",
    "    t=nlp(token[0])\n",
    "    for t2 in t:\n",
    "        t1=str(t2)\n",
    "        if t1 not in string.punctuation and t1 not in noises:\n",
    "            x2.append(t2.lemma_)\n",
    "    allTokens1.append(x2)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(allTokens[0:4])\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(allTokens1[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.fit(allTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTokenstrain=preprocess(X_train[:,0])\n",
    "allTokenstest=preprocess(X_test[:,0])\n",
    "\n",
    "Train_X_Tfidf = tfidf.transform(allTokenstrain)\n",
    "Test_X_Tfidf = tfidf.transform(allTokenstest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Train_X_Tfidf.shape)\n",
    "print(Test_X_Tfidf.shape)\n",
    "print(len(allTokenstrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "#y_train1[y_train==\"OFF\" ] = 1 \n",
    "#y_train1[y_train==\"NOT\"]=0\n",
    "\n",
    "#y_test[y_test==\"OFF\" ] = 1 \n",
    "#y_test[y_test==\"NOT\"]=0\n",
    "#Y_train=y_train.flatten()\n",
    "#Y_test=y_test.flatten()\n",
    "#print(Train_X_Tfidf.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(Train_X_Tfidf.shape)\n",
    "yTrain=le.fit_transform(y_train.flatten())\n",
    "print(yTrain.shape)\n",
    "print(le.classes_)\n",
    "yTest=le.fit_transform(y_test.flatten())\n",
    "\n",
    "print(le.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "SVM = SVC(class_weight='balanced')\n",
    "SVM=GridSearchCV(SVM,parameters)\n",
    "SVM.fit(Train_X_Tfidf,yTrain)\n",
    "SVM=SVM.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "\n",
    "print(\"SVM f1 score -> \",f1_score(predictions_SVM, yTest))\n",
    "print(\"SVM accuracy score\",accuracy_score(predictions_SVM,yTest )*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "classifier = RandomForestClassifier(max_depth=800, min_samples_split=5)\n",
    "params = {'n_estimators': [n for n in range(50,200,50)], 'criterion':['gini','entropy'], }\n",
    "classifier = GridSearchCV(classifier, params, cv=3, n_jobs=4)\n",
    "classifier.fit(Train_X_Tfidf,yTrain)\n",
    "classifier = classifier.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = classifier.predict(Test_X_Tfidf)\n",
    "\n",
    "print(\"Random Forest f1 score -> \",f1_score(test_predictions, yTest))\n",
    "print(\"Random Forest score>\",accuracy_score(test_predictions,yTest )*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "classifier = MultinomialNB(alpha=0.7)\n",
    "classifier.fit(Train_X_Tfidf,yTrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = classifier.predict(Test_X_Tfidf)\n",
    "\n",
    "print(\"Naive Bayes f1 score -> \",f1_score(test_predictions, yTest))\n",
    "print(\"Naive Bayes score>\",accuracy_score(test_predictions,yTest )*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clflr = LogisticRegression(multi_class='auto', solver='newton-cg',class_weight = 'balanced')\n",
    "classifier = GridSearchCV(classifier, {\"C\":np.logspace(-3,3,7), \"penalty\":[\"l2\"]}, cv=3, n_jobs=4)\n",
    "classifier.fit(Train_X_Tfidf,yTrain)\n",
    "classifier = classifier.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = classifier.predict(Test_X_Tfidf)\n",
    "\n",
    "print(\"Logistic Regression f1 score -> \",f1_score(test_predictions, yTest))\n",
    "print(\"Logistic Regression score>\",accuracy_score(test_predictions,yTest )*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_jobs=4)\n",
    "params = {'n_neighbors': [3,5,7,9], 'weights':['uniform', 'distance']}\n",
    "classifier = GridSearchCV(classifier, params, cv=3, n_jobs=4)\n",
    "classifier.fit(Train_X_Tfidf,yTrain)\n",
    "classifier = classifier.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = classifier.predict(Test_X_Tfidf)\n",
    "\n",
    "print(\"K nearest Neighbours f1 score -> \",f1_score(test_predictions, yTest))\n",
    "print(\"K nearest Neighbours score>\",accuracy_score(test_predictions,yTest )*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
