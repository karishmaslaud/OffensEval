{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 64
    },
    "colab_type": "code",
    "id": "T1HZ3sJ-t1SC",
    "outputId": "0856c108-c29a-477c-8ae1-cf832c4baa1b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#all imports\n",
    "import tensorflow as tf\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iCcqa_7et4cD",
    "outputId": "1764f7aa-7af0-4a83-bd69-a9abf39b2296"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at :/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gpuname=tf.test.gpu_device_name()\n",
    "if gpuname=='/device:GPU:0':\n",
    "  print('Found GPU at :{}'.format(gpuname))\n",
    "else:\n",
    "  raise(SystemError('GPU device not found'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "qNFEaLvKt-Zt",
    "outputId": "96e63c75-d651-4649-c294-5242eeb2a44c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU DEVICES available \n",
      "The device name is Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device=torch.device(\"cuda\")\n",
    "  print(\"There are %d GPU DEVICES available \" %torch.cuda.device_count())\n",
    "  print(\"The device name is %s\"%torch.cuda.get_device_name(0))\n",
    "else:\n",
    "  print(\"No GPU available using only CPU instead\")\n",
    "  device=torch.device(\"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BLjWz-0TuCbl",
    "outputId": "32ab33d4-8187-4dba-ff28-821277da0db9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "lddBU1JluKTa",
    "outputId": "813d5d75-47e2-4442-8074-64518d4f09e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace Greek/offenseval-greek-training-v1.tsv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
      "replace Greek/readme-trainingset-greek.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
     ]
    }
   ],
   "source": [
    "!unzip -P ****** -qq '/content/drive/My Drive/GreekData/Greek.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XAUepT0HuPeC"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1HB7ngrnscIG"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "colab_type": "code",
    "id": "uVXNdz8auhGR",
    "outputId": "a2f7812b-434d-482d-b669-01bb109097b0"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DgkzEygbscIY"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer=BertTokenizer.from_pretrained('bert-base-multilingual-cased',do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Q37nS_6yscIl",
    "outputId": "74fc0498-9f1f-40fc-847e-08439570eea8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119547"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "K5V9RsaxscIw",
    "outputId": "85d5f797-dd19-47af-e446-bb2a1207481a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hell', '##o', 'world', 'how', 'are', 'you', '?']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize('Hello WORLD how ARE yoU?')\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AyO6rrxescI6",
    "outputId": "bbab41e7-f838-4a3c-e445-c3c4cf1d05cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[61694, 10133, 11356, 14796, 10301, 13028, 136]\n"
     ]
    }
   ],
   "source": [
    "indexes = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "R_e-oxPiscJC",
    "outputId": "1dae5c84-13c4-4622-9516-7c5a5e7a8fef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] [SEP] [PAD] [UNK]\n"
     ]
    }
   ],
   "source": [
    "init_token = tokenizer.cls_token\n",
    "eos_token = tokenizer.sep_token\n",
    "pad_token = tokenizer.pad_token\n",
    "unk_token = tokenizer.unk_token\n",
    "\n",
    "print(init_token, eos_token, pad_token, unk_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bIDGEuatscJM",
    "outputId": "cf7fcbc3-5e74-4cc5-e0ec-9cb9bf0d3709"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 102 0 100\n"
     ]
    }
   ],
   "source": [
    "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
    "eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
    "\n",
    "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "i8xtYY9EscJV",
    "outputId": "d9cd75cf-2307-46b4-a741-4f4f5901ae04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 102 0 100\n"
     ]
    }
   ],
   "source": [
    "init_token_idx = tokenizer.cls_token_id\n",
    "eos_token_idx = tokenizer.sep_token_id\n",
    "pad_token_idx = tokenizer.pad_token_id\n",
    "unk_token_idx = tokenizer.unk_token_id\n",
    "\n",
    "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3jBiYT5MscJf",
    "outputId": "eb80be1b-3f3e-406c-c2d6-485e43c13382"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "max_input_length = tokenizer.max_model_input_sizes['bert-base-multilingual-cased']\n",
    "\n",
    "print(max_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-N5RmlcYscJm"
   },
   "outputs": [],
   "source": [
    "def tokenize_and_cut(sentence):\n",
    "    '''if z not in string.punctuation  and t.is_stop==False:\n",
    "                x2.append(z)\n",
    "                   \n",
    "    sentence1=' '.join((x2))  '''  \n",
    "    tokens = tokenizer.tokenize(sentence) \n",
    "    tokens = tokens[:max_input_length-2]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vICs3ikTJ1LN"
   },
   "outputs": [],
   "source": [
    "\n",
    "          \n",
    "def preprocess1(arrt):\n",
    "    doc=[]\n",
    "    emojis={}\n",
    "    #noises = ['@USER','n\\'t', '\\'s', '\\'m',\"’\"]\n",
    "    allTokens =[]\n",
    "    for txt in arrt:\n",
    "        k=0\n",
    "        sentVect=[]\n",
    "        txt=p2.clean(txt)\n",
    "        txt=p1.sub(\"\",txt)\n",
    "        x1=nlp(txt)\n",
    "        x2=[]\n",
    "        for t in x1:\n",
    "            z=str(t)\n",
    "            if z not in string.punctuation  and t.is_stop==False:\n",
    "                x2.append(z)\n",
    "                   \n",
    "        \n",
    "        sentence=' '.join((x2))         \n",
    "        allTokens.append(sentence)      \n",
    "    return allTokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d-IQaMo9scJv"
   },
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "\n",
    "TEXT = data.Field(batch_first = True,\n",
    "                  use_vocab = False,\n",
    "                  tokenize = tokenize_and_cut,\n",
    "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "                  init_token = init_token_idx,\n",
    "                  eos_token = eos_token_idx,\n",
    "                  pad_token = pad_token_idx,\n",
    "                  unk_token = unk_token_idx)\n",
    "\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_VXI0Qa-wy7w"
   },
   "outputs": [],
   "source": [
    "#GET THE DATA FROM THE PANDAS FRAME\n",
    "headers=['id','tweet','subtask_a']\n",
    "greekdata = pd.read_csv(\"Greek/offenseval-greek-training-v1.tsv\", delimiter='\\t',names=headers)\n",
    "data=greekdata[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "id": "t_kfrfapw91l",
    "outputId": "c37ec6bd-6e2a-4038-ca9e-34531865d7c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id                                              tweet subtask_a\n",
      "2002  4347  @USER Μη με κουρδίζεις νυχτιάτικα, βαλτός είσα...       NOT\n",
      "4645  4175                         Ωραιο μουνι, ποδοσφαιρικο.       OFF\n",
      "1916  1439  @USER 1. Αποκλείστηκες διότι με έβαλες σε ομάδ...       NOT\n",
      "1550  6404                 Κόψτε τους τα μανιτάρια #XFactorGR       NOT\n",
      "2358  5730  Σακης... Χριστινακι μου γλυκο! στα καπακια εισ...       NOT\n",
      "        id                                              tweet subtask_a\n",
      "4538  2263  @USER Mπλοκ το Μαράκι, δεν της άρεσε η λύση πο...       NOT\n",
      "7353  1055  @USER αδελφέ, δεν ξέρω τι ακριβώς έγινε.. απλά...       NOT\n",
      "3181  7142  Τα γαλλικά, ήταν η γλώσσα της Αριστοκρατίας τό...       NOT\n",
      "1626  3768  @USER Ρε Κουλη σε ξέρουμε επειδή φαίνεσαι και ...       NOT\n",
      "7634  5698  Από το πρωί είμαι με ένα «τι κοιτάς ρε μαλακισ...       OFF\n"
     ]
    }
   ],
   "source": [
    "greektrain,greektest= train_test_split(greekdata, test_size=0.2, random_state=42)\n",
    "export_csv = greektrain.to_csv ('Greek/TrainFilegreek.csv', index = None, header=True)\n",
    "print (greektrain.head())\n",
    "export_csv = greektest.to_csv ('Greek/TestFilegreek.csv', index = None, header=True)\n",
    "print (greektest.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6vMyDTjoscJ2"
   },
   "outputs": [],
   "source": [
    "from torchtext import datasets\n",
    "from torchtext import data\n",
    "\n",
    "train_val_fields = [\n",
    "    ('id', None), # we dont need this, so no processing\n",
    "    ('tweet', TEXT), # process it as label\n",
    "    ('subtask_a', LABEL), # we dont need this, so no processing\n",
    "]\n",
    "train_data, valid_data = data.TabularDataset.splits(path='Greek/', \n",
    "                                            format='csv', \n",
    "                                            train='TrainFilegreek.csv', \n",
    "                                            validation='TestFilegreek.csv', \n",
    "                                            fields=train_val_fields, \n",
    "                                            skip_header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "SBYyyNyFscJ7",
    "outputId": "019df8e3-d023-4b5a-f304-d06797c92f4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 6995\n",
      "Number of validation examples: 1749\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train_data)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "YuIRlHjTscKC",
    "outputId": "de3eec49-264b-4382-da37-5c51a584babb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tweet': [476, 13517, 62152, 10484, 466, 97317, 33617, 31796, 13140, 70887, 43202, 31821, 465, 31625, 10649, 483, 29223, 14669, 35670, 45878, 27393, 465, 24767, 107, 471, 480, 16146, 31712, 18015, 469, 17198, 30645, 64472, 27621, 108, 22013, 70279, 53166, 487, 13140, 63444, 61399, 61399, 107, 108, 22013, 70279, 53166, 168, 28174, 12961, 38278], 'subtask_a': 'NOT'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "n2YLmUNGscKI",
    "outputId": "56952338-9292-49d6-e6be-49e2165d1498"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['μ', '##ο', '##λι', '##ς', 'β', '##γη', '##κε', 'αν', '##α', '##κο', '##ιν', '##ωση', 'α', '##πο', 'την', 'σ', '##π', '##υ', '##ρι', '##δου', '##λα', 'α', '##κα', '\"', 'η', 'π', '##ρ', '##ω', '##τη', 'ε', '##λ', '##λη', '##νι', '##δα', '#', 'master', '##chef', '##gr', 'χ', '##α', '##χ', '##χα', '##χα', '\"', '#', 'master', '##chef', '##gr', '_', 'xe', '##ft', '##iles']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(vars(train_data.examples[6])['tweet'])\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q9lW7T9oscKP"
   },
   "outputs": [],
   "source": [
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "NUxtDneZscKU",
    "outputId": "42052c55-f40d-473c-deea-c876b78e67c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function _default_unk_index at 0x7f9a84579d90>, {'NOT': 0, 'OFF': 1})\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G0bCJlCrscKb"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    sort=False,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P7sdPGVtscKg"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import BertConfig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wg5pniU7duZA"
   },
   "outputs": [],
   "source": [
    "config1=BertConfig.from_pretrained('bert-base-multilingual-cased',output_hidden_states=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kT5N2vZRgpyr"
   },
   "outputs": [],
   "source": [
    "bert =BertModel.from_pretrained('bert-base-multilingual-cased',config=config1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LS8KPV7WscKl"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import BertConfig\n",
    "class BERTGRUSentiment(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                 n_layers,\n",
    "                 bidirectional,\n",
    "                 dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert = bert\n",
    "        \n",
    "        embedding_dim = bert.config.to_dict()['hidden_size']*4\n",
    "        \n",
    "        self.rnn = nn.GRU(embedding_dim,\n",
    "                          hidden_dim,\n",
    "                          num_layers = n_layers,\n",
    "                          bidirectional = bidirectional,\n",
    "                          batch_first = True,\n",
    "                          dropout = 0 if n_layers < 2 else dropout)\n",
    "        \n",
    "        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        #text = [batch size, sent len]\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            embedded= self.bert(text)\n",
    "        hidden=embedded[-1]\n",
    "        #print(hidden[12].size())\n",
    "        #print(embedded[0].size())\n",
    "        #print(len(hidden))\n",
    "        '''\n",
    "        encoded_layers=hidden\n",
    "        print (\"Number of layers:\", len(encoded_layers))\n",
    "        layer_i = 0\n",
    "\n",
    "        print (\"Number of batches:\", len(encoded_layers[layer_i]))\n",
    "        batch_i = 0\n",
    "\n",
    "        print (\"Number of tokens:\", len(encoded_layers[layer_i][batch_i]))\n",
    "        token_i = 0\n",
    "\n",
    "        print (\"Number of hidden units:\", len(encoded_layers[layer_i][batch_i][token_i]))\n",
    "        '''\n",
    "\n",
    "        token_embeddings=torch.stack(hidden, dim=0)\n",
    "        #print(token_embeddings.size())\n",
    "        token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "        #print(token_embeddings.size())\n",
    "        token_embeddings = token_embeddings.permute(1,2,0,3)\n",
    "        #print(token_embeddings.size())\n",
    "        token_vecs_cat = []\n",
    "        irnn=[]\n",
    "        for b in token_embeddings:\n",
    "          token_vecs_cat = []\n",
    "          for token in b:\n",
    "           cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "           #print(\"cat_vec\",len(cat_vec))\n",
    "           token_vecs_cat.append(cat_vec)\n",
    "           #a=torch.Tensor(token_vecs_cat)\n",
    "           #print(len(token_vecs_cat))\n",
    "          a = torch.stack(token_vecs_cat)#.to(device)\n",
    "          irnn.append(a) \n",
    "        z=torch.stack(irnn)#.to(device)\n",
    "        '''print(z.size())\n",
    "        print ('Shape is: %d x %d x %d' % (len(z), len(z[0]),len(z[1])))\n",
    "        print(type(z))\n",
    "        print(type(z[0]))\n",
    "        print(type(z[1]))\n",
    "        '''\n",
    "        _, hidden = self.rnn(z)\n",
    "        \n",
    "        #hidden = [n layers * n directions, batch size, emb dim]\n",
    "        \n",
    "        if self.rnn.bidirectional:\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "                \n",
    "        #hidden = [batch size, hid dim]\n",
    "        \n",
    "        output = self.out(hidden)\n",
    "        \n",
    "        #output = [batch size, out dim]\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "jWcm1RM8yktU",
    "outputId": "a1d2db2e-7840-4940-c5fd-f80c8f8c7ada"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 2., 3.],\n",
      "         [2., 3., 4.]],\n",
      "\n",
      "        [[1., 2., 3.],\n",
      "         [2., 3., 4.]]])\n",
      "torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a=[[[1,2,3],[2,3,4]],[[1,2,3],[2,3,4]]]\n",
    "b=torch.Tensor(a)\n",
    "print(b)\n",
    "print(b.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FPTyANrGdmx2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fEIqrIG8scKq"
   },
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 512\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.25\n",
    "\n",
    "model = BERTGRUSentiment(bert,\n",
    "                         HIDDEN_DIM,\n",
    "                         OUTPUT_DIM,\n",
    "                         N_LAYERS,\n",
    "                         BIDIRECTIONAL,\n",
    "                         DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sbyI1h18scKy",
    "outputId": "483b0319-eaf2-4f49-da7c-ce751d2fab8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 193,595,393 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QkYmbhy9scK3"
   },
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():                \n",
    "    if name.startswith('bert'):\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "kJIl5tYVscK8",
    "outputId": "64099ae6-8750-4b19-8436-d36cf96cb327"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 15,741,953 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "colab_type": "code",
    "id": "c2QcdD9uscLB",
    "outputId": "52b67927-9922-4e3d-fa2a-167d075624af"
   },
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():                \n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5HCgrge6scLH"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from transformers import AdamW\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZVXs0n9UscLJ"
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hxgQxlJXscLO"
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SEU_NUOOscLS"
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "quYVUygE2u4v"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def binary_accuracy2(preds, y):\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    #print(\"Predicted\",rounded_preds)\n",
    "    #print(\"labelsFlattend\",y)\n",
    "    return f1_score(y.detach().cpu().numpy(), rounded_preds.detach().cpu().numpy(), average='macro'),acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HjVM0U8kscLW"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        #print(batch)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.tweet).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.subtask_a)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.subtask_a)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uhjbtc1cscLY"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    nb_eval_steps=0\n",
    "    eval_acc=0\n",
    "    eval_f1=0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.tweet).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.subtask_a)\n",
    "            \n",
    "            tmpf1score,acc = binary_accuracy2(predictions, batch.subtask_a)\n",
    "            eval_f1 = eval_f1+tmpf1score\n",
    "            eval_acc=eval_acc+acc\n",
    "            nb_eval_steps += 1\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    print(\"  F1 score: {0:.3f}\".format(eval_f1/nb_eval_steps))\n",
    "    print(\"  Accuracy score: {0:.3f}\".format(eval_acc/nb_eval_steps))\n",
    " \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H1HkVDIc6rjh"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    nb_eval_steps=0\n",
    "    eval_acc=0\n",
    "    eval_f1=0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.tweet).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.subtask_a)\n",
    "            \n",
    "            tmpf1score,acc = binary_accuracy2(predictions, batch.subtask_a)\n",
    "            eval_f1 = eval_f1+tmpf1score\n",
    "            eval_acc=eval_acc+acc\n",
    "            nb_eval_steps += 1\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    print(\"  F1 score: {0:.3f}\".format(eval_f1/nb_eval_steps))\n",
    "    print(\"  Accuracy score: {0:.3f}\".format(eval_acc/nb_eval_steps))\n",
    " \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "948kFVunscLs"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "EuZJAXIOscLy",
    "outputId": "eb50b918-4115-44a3-aeda-5f30dd0662b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 144, 768])\n",
      "torch.Size([128, 157, 768])\n",
      "torch.Size([128, 155, 768])\n",
      "torch.Size([128, 158, 768])\n",
      "torch.Size([128, 148, 768])\n",
      "torch.Size([128, 139, 768])\n",
      "torch.Size([128, 152, 768])\n",
      "torch.Size([128, 136, 768])\n",
      "torch.Size([128, 143, 768])\n",
      "torch.Size([128, 149, 768])\n",
      "torch.Size([128, 152, 768])\n",
      "torch.Size([128, 150, 768])\n",
      "torch.Size([128, 168, 768])\n",
      "torch.Size([128, 153, 768])\n",
      "torch.Size([128, 166, 768])\n",
      "torch.Size([128, 151, 768])\n",
      "torch.Size([128, 160, 768])\n",
      "torch.Size([128, 162, 768])\n",
      "torch.Size([128, 142, 768])\n",
      "torch.Size([128, 147, 768])\n",
      "torch.Size([128, 141, 768])\n",
      "torch.Size([128, 143, 768])\n",
      "torch.Size([128, 148, 768])\n",
      "torch.Size([128, 150, 768])\n",
      "torch.Size([83, 145, 768])\n",
      "torch.Size([128, 147, 768])\n",
      "torch.Size([128, 150, 768])\n",
      "torch.Size([128, 139, 768])\n",
      "torch.Size([128, 141, 768])\n",
      "torch.Size([128, 151, 768])\n",
      "torch.Size([128, 167, 768])\n",
      "torch.Size([128, 144, 768])\n",
      "torch.Size([128, 143, 768])\n",
      "torch.Size([128, 141, 768])\n",
      "torch.Size([128, 149, 768])\n",
      "torch.Size([128, 152, 768])\n",
      "torch.Size([128, 146, 768])\n",
      "torch.Size([128, 156, 768])\n",
      "torch.Size([128, 142, 768])\n",
      "torch.Size([128, 154, 768])\n",
      "torch.Size([128, 150, 768])\n",
      "torch.Size([128, 147, 768])\n",
      "torch.Size([128, 151, 768])\n",
      "torch.Size([128, 149, 768])\n",
      "torch.Size([128, 169, 768])\n",
      "torch.Size([128, 149, 768])\n",
      "torch.Size([128, 150, 768])\n",
      "torch.Size([128, 139, 768])\n",
      "torch.Size([128, 149, 768])\n",
      "torch.Size([128, 149, 768])\n",
      "torch.Size([128, 150, 768])\n",
      "torch.Size([128, 164, 768])\n",
      "torch.Size([128, 153, 768])\n",
      "torch.Size([128, 147, 768])\n",
      "torch.Size([128, 145, 768])\n",
      "torch.Size([128, 148, 768])\n",
      "torch.Size([128, 147, 768])\n",
      "torch.Size([128, 142, 768])\n",
      "torch.Size([128, 149, 768])\n",
      "torch.Size([128, 151, 768])\n",
      "torch.Size([128, 148, 768])\n",
      "torch.Size([128, 155, 768])\n",
      "torch.Size([128, 147, 768])\n",
      "torch.Size([128, 145, 768])\n",
      "torch.Size([128, 151, 768])\n",
      "torch.Size([128, 146, 768])\n",
      "torch.Size([128, 145, 768])\n",
      "torch.Size([128, 144, 768])\n",
      "torch.Size([85, 139, 768])\n",
      "  F1 score: 0.465\n",
      "  Accuracy score: 0.728\n",
      "Epoch: 01 | Epoch Time: 2m 11s\n",
      "\tTrain Loss: 0.682 | Train Acc: 67.68%\n",
      "\t Val. Loss: 0.573 |  Val. Acc: 72.85%\n",
      "torch.Size([128, 146, 768])\n",
      "torch.Size([128, 148, 768])\n",
      "torch.Size([128, 150, 768])\n",
      "torch.Size([128, 151, 768])\n",
      "torch.Size([128, 154, 768])\n",
      "torch.Size([128, 149, 768])\n",
      "torch.Size([128, 151, 768])\n",
      "torch.Size([128, 150, 768])\n",
      "torch.Size([128, 149, 768])\n",
      "torch.Size([128, 149, 768])\n",
      "torch.Size([128, 146, 768])\n",
      "torch.Size([128, 153, 768])\n",
      "torch.Size([128, 153, 768])\n",
      "torch.Size([128, 144, 768])\n",
      "torch.Size([128, 146, 768])\n",
      "torch.Size([128, 139, 768])\n",
      "torch.Size([128, 152, 768])\n",
      "torch.Size([128, 167, 768])\n",
      "torch.Size([128, 147, 768])\n",
      "torch.Size([128, 142, 768])\n",
      "torch.Size([128, 168, 768])\n",
      "torch.Size([128, 156, 768])\n",
      "torch.Size([128, 139, 768])\n",
      "torch.Size([128, 142, 768])\n",
      "torch.Size([128, 162, 768])\n",
      "torch.Size([128, 166, 768])\n",
      "torch.Size([128, 169, 768])\n",
      "torch.Size([128, 143, 768])\n",
      "torch.Size([128, 158, 768])\n",
      "torch.Size([83, 157, 768])\n",
      "torch.Size([128, 152, 768])\n",
      "torch.Size([128, 143, 768])\n",
      "torch.Size([128, 150, 768])\n",
      "torch.Size([128, 152, 768])\n",
      "torch.Size([128, 141, 768])\n",
      "torch.Size([128, 155, 768])\n",
      "torch.Size([128, 149, 768])\n",
      "torch.Size([128, 144, 768])\n",
      "torch.Size([128, 151, 768])\n",
      "torch.Size([128, 143, 768])\n",
      "torch.Size([128, 150, 768])\n",
      "torch.Size([128, 150, 768])\n",
      "torch.Size([128, 160, 768])\n",
      "torch.Size([128, 143, 768])\n",
      "torch.Size([128, 143, 768])\n",
      "torch.Size([128, 149, 768])\n",
      "torch.Size([128, 154, 768])\n",
      "torch.Size([128, 151, 768])\n",
      "torch.Size([128, 142, 768])\n",
      "torch.Size([128, 147, 768])\n",
      "torch.Size([128, 145, 768])\n",
      "torch.Size([128, 138, 768])\n",
      "torch.Size([128, 156, 768])\n",
      "torch.Size([128, 150, 768])\n",
      "torch.Size([128, 140, 768])\n",
      "torch.Size([128, 148, 768])\n",
      "torch.Size([128, 147, 768])\n",
      "torch.Size([128, 142, 768])\n",
      "torch.Size([128, 149, 768])\n",
      "torch.Size([128, 151, 768])\n",
      "torch.Size([128, 148, 768])\n",
      "torch.Size([128, 155, 768])\n",
      "torch.Size([128, 147, 768])\n",
      "torch.Size([128, 145, 768])\n",
      "torch.Size([128, 151, 768])\n",
      "torch.Size([128, 146, 768])\n",
      "torch.Size([128, 145, 768])\n",
      "torch.Size([128, 144, 768])\n",
      "torch.Size([85, 139, 768])\n",
      "  F1 score: 0.454\n",
      "  Accuracy score: 0.739\n",
      "Epoch: 02 | Epoch Time: 2m 8s\n",
      "\tTrain Loss: 0.580 | Train Acc: 71.03%\n",
      "\t Val. Loss: 0.533 |  Val. Acc: 73.88%\n",
      "torch.Size([128, 143, 768])\n",
      "torch.Size([128, 147, 768])\n",
      "torch.Size([128, 156, 768])\n",
      "torch.Size([128, 150, 768])\n",
      "torch.Size([128, 166, 768])\n",
      "torch.Size([128, 139, 768])\n",
      "torch.Size([128, 162, 768])\n",
      "torch.Size([128, 139, 768])\n",
      "torch.Size([128, 146, 768])\n",
      "torch.Size([128, 149, 768])\n",
      "torch.Size([128, 156, 768])\n",
      "torch.Size([128, 151, 768])\n",
      "torch.Size([128, 139, 768])\n",
      "torch.Size([128, 158, 768])\n",
      "torch.Size([128, 153, 768])\n",
      "torch.Size([128, 150, 768])\n",
      "torch.Size([128, 148, 768])\n",
      "torch.Size([128, 155, 768])\n",
      "torch.Size([128, 149, 768])\n",
      "torch.Size([128, 151, 768])\n",
      "torch.Size([128, 146, 768])\n",
      "torch.Size([128, 149, 768])\n",
      "torch.Size([128, 169, 768])\n",
      "torch.Size([128, 152, 768])\n",
      "torch.Size([128, 145, 768])\n",
      "torch.Size([128, 141, 768])\n",
      "torch.Size([128, 146, 768])\n",
      "torch.Size([128, 149, 768])\n",
      "torch.Size([128, 149, 768])\n",
      "torch.Size([128, 140, 768])\n",
      "torch.Size([128, 146, 768])\n",
      "torch.Size([128, 148, 768])\n",
      "torch.Size([128, 153, 768])\n",
      "torch.Size([128, 137, 768])\n",
      "torch.Size([128, 142, 768])\n",
      "torch.Size([83, 145, 768])\n",
      "torch.Size([128, 149, 768])\n",
      "torch.Size([128, 150, 768])\n",
      "torch.Size([128, 164, 768])\n",
      "torch.Size([128, 154, 768])\n",
      "torch.Size([128, 168, 768])\n",
      "torch.Size([128, 150, 768])\n",
      "torch.Size([128, 139, 768])\n",
      "torch.Size([128, 150, 768])\n",
      "torch.Size([128, 144, 768])\n",
      "torch.Size([128, 147, 768])\n",
      "torch.Size([128, 167, 768])\n",
      "torch.Size([128, 150, 768])\n",
      "torch.Size([128, 154, 768])\n",
      "torch.Size([128, 160, 768])\n",
      "torch.Size([128, 146, 768])\n",
      "torch.Size([128, 149, 768])\n",
      "torch.Size([128, 156, 768])\n",
      "torch.Size([128, 143, 768])\n",
      "torch.Size([128, 145, 768])\n",
      "torch.Size([128, 148, 768])\n",
      "torch.Size([128, 147, 768])\n",
      "torch.Size([128, 142, 768])\n",
      "torch.Size([128, 149, 768])\n",
      "torch.Size([128, 151, 768])\n",
      "torch.Size([128, 148, 768])\n",
      "torch.Size([128, 155, 768])\n",
      "torch.Size([128, 147, 768])\n",
      "torch.Size([128, 145, 768])\n",
      "torch.Size([128, 151, 768])\n",
      "torch.Size([128, 146, 768])\n",
      "torch.Size([128, 145, 768])\n",
      "torch.Size([128, 144, 768])\n",
      "torch.Size([85, 139, 768])\n",
      "  F1 score: 0.736\n",
      "  Accuracy score: 0.827\n",
      "Epoch: 03 | Epoch Time: 2m 8s\n",
      "\tTrain Loss: 0.505 | Train Acc: 77.08%\n",
      "\t Val. Loss: 0.428 |  Val. Acc: 82.72%\n",
      "torch.Size([128, 149, 768])\n",
      "torch.Size([128, 147, 768])\n",
      "torch.Size([128, 152, 768])\n",
      "torch.Size([128, 148, 768])\n",
      "torch.Size([128, 155, 768])\n",
      "torch.Size([128, 169, 768])\n",
      "torch.Size([128, 146, 768])\n",
      "torch.Size([128, 151, 768])\n",
      "torch.Size([128, 147, 768])\n",
      "torch.Size([128, 145, 768])\n",
      "torch.Size([128, 139, 768])\n",
      "torch.Size([128, 154, 768])\n",
      "torch.Size([128, 139, 768])\n",
      "torch.Size([128, 144, 768])\n",
      "torch.Size([128, 149, 768])\n",
      "torch.Size([83, 150, 768])\n",
      "torch.Size([128, 164, 768])\n",
      "torch.Size([128, 144, 768])\n",
      "torch.Size([128, 150, 768])\n",
      "torch.Size([128, 144, 768])\n",
      "torch.Size([128, 149, 768])\n",
      "torch.Size([128, 146, 768])\n",
      "torch.Size([128, 147, 768])\n",
      "torch.Size([128, 150, 768])\n",
      "torch.Size([128, 151, 768])\n",
      "torch.Size([128, 147, 768])\n",
      "torch.Size([128, 141, 768])\n",
      "torch.Size([128, 145, 768])\n",
      "torch.Size([128, 149, 768])\n",
      "torch.Size([128, 166, 768])\n",
      "torch.Size([128, 156, 768])\n",
      "torch.Size([128, 158, 768])\n",
      "torch.Size([128, 147, 768])\n",
      "torch.Size([128, 143, 768])\n",
      "torch.Size([128, 146, 768])\n",
      "torch.Size([128, 150, 768])\n",
      "torch.Size([128, 150, 768])\n",
      "torch.Size([128, 137, 768])\n",
      "torch.Size([128, 140, 768])\n",
      "torch.Size([128, 150, 768])\n",
      "torch.Size([128, 151, 768])\n",
      "torch.Size([128, 149, 768])\n",
      "torch.Size([128, 151, 768])\n",
      "torch.Size([128, 154, 768])\n",
      "torch.Size([128, 148, 768])\n",
      "torch.Size([128, 150, 768])\n",
      "torch.Size([128, 156, 768])\n",
      "torch.Size([128, 167, 768])\n",
      "torch.Size([128, 156, 768])\n",
      "torch.Size([128, 150, 768])\n",
      "torch.Size([128, 147, 768])\n",
      "torch.Size([128, 168, 768])\n",
      "torch.Size([128, 146, 768])\n",
      "torch.Size([128, 162, 768])\n",
      "torch.Size([128, 152, 768])\n",
      "torch.Size([128, 148, 768])\n",
      "torch.Size([128, 147, 768])\n",
      "torch.Size([128, 142, 768])\n",
      "torch.Size([128, 149, 768])\n",
      "torch.Size([128, 151, 768])\n",
      "torch.Size([128, 148, 768])\n",
      "torch.Size([128, 155, 768])\n",
      "torch.Size([128, 147, 768])\n",
      "torch.Size([128, 145, 768])\n",
      "torch.Size([128, 151, 768])\n",
      "torch.Size([128, 146, 768])\n",
      "torch.Size([128, 145, 768])\n",
      "torch.Size([128, 144, 768])\n",
      "torch.Size([85, 139, 768])\n",
      "  F1 score: 0.721\n",
      "  Accuracy score: 0.828\n",
      "Epoch: 04 | Epoch Time: 2m 7s\n",
      "\tTrain Loss: 0.444 | Train Acc: 81.23%\n",
      "\t Val. Loss: 0.436 |  Val. Acc: 82.84%\n",
      "torch.Size([128, 143, 768])\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 20\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "        \n",
    "    end_time = time.time()\n",
    "        \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "    #if valid_loss < best_valid_loss:\n",
    "    #    best_valid_loss = valid_loss\n",
    "    #    torch.save(model.state_dict(), '/content/drive/My Drive/GreekData/bertgreekgru.pth.tar')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4KhuQl0yscL3"
   },
   "outputs": [],
   "source": [
    "torch.save({'state_dict': model.state_dict()}, '/content/drive/My Drive/GreekData/bertgreekgru.pth.tar')\n",
    "checkpoint = torch.load('/content/drive/My Drive/GreekData/bertgreekgru.pth.tar')\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "X",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
