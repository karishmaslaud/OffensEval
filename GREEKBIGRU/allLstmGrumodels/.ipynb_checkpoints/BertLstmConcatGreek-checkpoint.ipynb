{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wpVuAaLWCq39"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T1HZ3sJ-t1SC"
   },
   "outputs": [],
   "source": [
    "#all imports\n",
    "import tensorflow as tf\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iCcqa_7et4cD",
    "outputId": "7a4c6477-c13e-458d-bb6f-ce62a326c812"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at :/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gpuname=tf.test.gpu_device_name()\n",
    "if gpuname=='/device:GPU:0':\n",
    "  print('Found GPU at :{}'.format(gpuname))\n",
    "else:\n",
    "  raise(SystemError('GPU device not found'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "qNFEaLvKt-Zt",
    "outputId": "bb7447e0-8e80-4e3d-ce22-2583c9e8b9ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU DEVICES available \n",
      "The device name is Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device=torch.device(\"cuda\")\n",
    "  print(\"There are %d GPU DEVICES available \" %torch.cuda.device_count())\n",
    "  print(\"The device name is %s\"%torch.cuda.get_device_name(0))\n",
    "else:\n",
    "  print(\"No GPU available using only CPU instead\")\n",
    "  device=torch.device(\"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "BLjWz-0TuCbl",
    "outputId": "b5e4c6ea-7c14-42c2-8c18-655bfbde7165"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lddBU1JluKTa"
   },
   "outputs": [],
   "source": [
    "!unzip -P yourpassword -qq '/content/drive/My Drive/GreekData/Greek.zip'\n",
    "!unzip -P yourpassword -qq '/content/drive/My Drive/GreekData/offenseval2020-test-greek.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XAUepT0HuPeC"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1HB7ngrnscIG"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 677
    },
    "colab_type": "code",
    "id": "uVXNdz8auhGR",
    "outputId": "f01c611f-4e60-4278-dbae-c00be8bf097e"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "fc9811be1f5f4952aeb6cab2e92cf5e1",
      "2d37d4835db54c5fa4cc0e7f638647d4",
      "d886bdaa66134c4294f7f4a6ebbd51c5",
      "18c46b8340ac4d25980038762886467d",
      "8185ba43fc5e410e908733860972ba6e",
      "cc5e498a5e0648279cec5af0f374105c",
      "be2b53d3b6364a29a860507c7ee292cf",
      "69fc71f8bd444365aaf99f78251d7279"
     ]
    },
    "colab_type": "code",
    "id": "DgkzEygbscIY",
    "outputId": "4d738806-162f-44de-9a5e-cc8569d1b964",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc9811be1f5f4952aeb6cab2e92cf5e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=995526, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer=BertTokenizer.from_pretrained('bert-base-multilingual-cased',do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "R_e-oxPiscJC",
    "outputId": "dc191b75-398b-4422-cb93-1a2c7b577673"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] [SEP] [PAD] [UNK]\n"
     ]
    }
   ],
   "source": [
    "init_token = tokenizer.cls_token\n",
    "eos_token = tokenizer.sep_token\n",
    "pad_token = tokenizer.pad_token\n",
    "unk_token = tokenizer.unk_token\n",
    "\n",
    "print(init_token, eos_token, pad_token, unk_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bIDGEuatscJM",
    "outputId": "53c0e73a-0676-42c5-9ba2-c5b2d9869800"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 102 0 100\n"
     ]
    }
   ],
   "source": [
    "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
    "eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
    "\n",
    "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "i8xtYY9EscJV",
    "outputId": "ef9fb59e-9160-43bd-9fff-cfff8d2b643a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 102 0 100\n"
     ]
    }
   ],
   "source": [
    "init_token_idx = tokenizer.cls_token_id\n",
    "eos_token_idx = tokenizer.sep_token_id\n",
    "pad_token_idx = tokenizer.pad_token_id\n",
    "unk_token_idx = tokenizer.unk_token_id\n",
    "\n",
    "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3jBiYT5MscJf",
    "outputId": "490b4ccb-b5b8-4c1f-b158-0a3a9376df68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "max_input_length = tokenizer.max_model_input_sizes['bert-base-multilingual-cased']\n",
    "\n",
    "print(max_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "4pIvsO70Ok67",
    "outputId": "196864e2-473f-4579-baf4-eb63fd7355c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/8d/521be7f0091fe0f2ae690cc044faf43e3445e0ff33c574eae752dd7e39fa/emoji-0.5.4.tar.gz (43kB)\n",
      "\r",
      "\u001b[K     |███████▌                        | 10kB 19.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 20kB 3.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▋         | 30kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▏ | 40kB 2.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 51kB 2.7MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for emoji: filename=emoji-0.5.4-cp36-none-any.whl size=42176 sha256=390ab0325476a1408f55d8c537388b82c75d58ae123a4cd92e9088884c716b9a\n",
      "  Stored in directory: /root/.cache/pip/wheels/2a/a9/0a/4f8e8cce8074232aba240caca3fade315bb49fac68808d1a9c\n",
      "Successfully built emoji\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-0.5.4\n"
     ]
    }
   ],
   "source": [
    "pip install emoji --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "jbQ8PWrURW-_",
    "outputId": "20f950b5-e857-4b9b-eeb5-a6ca492889fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('el_core_news_md')\n"
     ]
    }
   ],
   "source": [
    "import spacy.cli\n",
    "spacy.cli.download(\"el_core_news_md\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Yr0OS6pRXC-"
   },
   "outputs": [],
   "source": [
    "#LEMMATIZATION\n",
    "import string\n",
    "import spacy\n",
    "#import el_core_news_sm \n",
    "from spacy.tokenizer import Tokenizer\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "nlp =  spacy.load('el_core_news_md')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-N5RmlcYscJm"
   },
   "outputs": [],
   "source": [
    "def tokenize_and_cut(sentence):\n",
    "    s=\"\"\n",
    "    txt1=emoji.demojize(sentence)\n",
    "    x1=nlp(txt1)\n",
    "    #print(x1)\n",
    "    x2=[]\n",
    "    for t in x1:\n",
    "        z=str(t)\n",
    "        if z not in string.punctuation  and t.is_stop==False:\n",
    "          x2.append(z)\n",
    "    s+=' '.join(x2)\n",
    "    #print(s)  \n",
    "    tokens = tokenizer.tokenize(s) \n",
    "    tokens = tokens[:max_input_length-2]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590
    },
    "colab_type": "code",
    "id": "vICs3ikTJ1LN",
    "outputId": "18786c2b-d11f-4804-de16-421db59d6d60"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d-IQaMo9scJv"
   },
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "\n",
    "TEXT = data.Field(batch_first = True,\n",
    "                  use_vocab = False,\n",
    "                  tokenize = tokenize_and_cut,\n",
    "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "                  init_token = init_token_idx,\n",
    "                  eos_token = eos_token_idx,\n",
    "                  pad_token = pad_token_idx,\n",
    "                  unk_token = unk_token_idx)\n",
    "\n",
    "\n",
    "LABEL = data.LabelField(dtype = torch.float)\n",
    "\n",
    "#TIDS=data.Field(use_vocab=False, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_VXI0Qa-wy7w"
   },
   "outputs": [],
   "source": [
    "#GET THE DATA FROM THE PANDAS FRAME\n",
    "headers=['id','tweet','subtask_a']\n",
    "greekdata = pd.read_csv(\"Greek/offenseval-greek-training-v1.tsv\", delimiter='\\t',names=headers)\n",
    "data=greekdata[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PonTY7lu5uqW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "id": "t_kfrfapw91l",
    "outputId": "3c99088a-6266-4a64-841f-d02c947deaf0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id                                              tweet subtask_a\n",
      "2002  4347  @USER Μη με κουρδίζεις νυχτιάτικα, βαλτός είσα...       NOT\n",
      "4645  4175                         Ωραιο μουνι, ποδοσφαιρικο.       OFF\n",
      "1916  1439  @USER 1. Αποκλείστηκες διότι με έβαλες σε ομάδ...       NOT\n",
      "1550  6404                 Κόψτε τους τα μανιτάρια #XFactorGR       NOT\n",
      "2358  5730  Σακης... Χριστινακι μου γλυκο! στα καπακια εισ...       NOT\n",
      "        id                                              tweet subtask_a\n",
      "4538  2263  @USER Mπλοκ το Μαράκι, δεν της άρεσε η λύση πο...       NOT\n",
      "7353  1055  @USER αδελφέ, δεν ξέρω τι ακριβώς έγινε.. απλά...       NOT\n",
      "3181  7142  Τα γαλλικά, ήταν η γλώσσα της Αριστοκρατίας τό...       NOT\n",
      "1626  3768  @USER Ρε Κουλη σε ξέρουμε επειδή φαίνεσαι και ...       NOT\n",
      "7634  5698  Από το πρωί είμαι με ένα «τι κοιτάς ρε μαλακισ...       OFF\n"
     ]
    }
   ],
   "source": [
    "greektrain,greektest= train_test_split(greekdata, test_size=0.2, random_state=42)\n",
    "export_csv = greektrain.to_csv ('Greek/TrainFilegreek.csv', index = None, header=True)\n",
    "print (greektrain.head())\n",
    "export_csv = greektest.to_csv ('Greek/TestFilegreek.csv', index = None, header=True)\n",
    "print (greektest.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W8488fpXoJ7Y"
   },
   "outputs": [],
   "source": [
    "ygiven=[]\n",
    "ypredicted=[]\n",
    "\n",
    "def convertToInt(val):\n",
    "    if not val:\n",
    "        return 0    \n",
    "    try:\n",
    "        return np.int64(val)\n",
    "    except:        \n",
    "        return np.int64(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "colab_type": "code",
    "id": "UcfDXn36bLxw",
    "outputId": "eba16566-03ba-4abc-c711-78223feb7a61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1544, 2)\n",
      "     id                                              tweet ypredicted\n",
      "0  2707  @USER Θέλω να των δω από εδώ και εμπρός αν δεν...        NOT\n",
      "1  2251  #survivorgr Α Και 60 φορές και με διαφορετικού...        NOT\n",
      "2  9814  Και μου έλεγε η γυναίκα μου το πρωί πάρε την τ...        NOT\n",
      "3  8949                   κατω τα χερια απο τον #κυρανακης        NOT\n",
      "4  6913  @USER μην μας το παιζεις πονοψυχη,κρυφορατσιστ...        NOT\n",
      "id             int64\n",
      "tweet         object\n",
      "ypredicted    object\n",
      "dtype: object\n",
      "(1544, 3)\n",
      "     id                                              tweet ypredicted\n",
      "0  2707  @USER Θέλω να των δω από εδώ και εμπρός αν δεν...        NOT\n",
      "1  2251  #survivorgr Α Και 60 φορές και με διαφορετικού...        NOT\n",
      "2  9814  Και μου έλεγε η γυναίκα μου το πρωί πάρε την τ...        NOT\n",
      "3  8949                   κατω τα χερια απο τον #κυρανακης        NOT\n",
      "4  6913  @USER μην μας το παιζεις πονοψυχη,κρυφορατσιστ...        NOT\n"
     ]
    }
   ],
   "source": [
    "headers=['id','ypredicted']\n",
    "greekdataBaseline = pd.read_csv(\"/content/drive/My Drive/OffensEvalGoldLabels/greek-goldlabels.csv\", delimiter=',',names=headers)\n",
    "#,converters={\"id\":convertToInt}       \n",
    "greekdataBaseline.id = greekdataBaseline.id.astype(int)\n",
    "#greekdataBaseline=greekdataBaseline[1:]\n",
    "#print(greekdataBaseline.dtypes)\n",
    "headers=['id','tweet']\n",
    "greekDataTest = pd.read_csv(\"testset_taska.tsv\", delimiter='\\t',names=headers,\n",
    "                              converters={\"id\":convertToInt})\n",
    "greekDataTest=greekDataTest[1:]\n",
    "#print(greekDataTest.head())\n",
    "#print(greekDataTest.dtypes)\n",
    "print(greekDataTest.shape)\n",
    "result = pd.merge(greekDataTest, greekdataBaseline, on='id', how='inner')\n",
    "print(result.head())\n",
    "print(result.dtypes)\n",
    "print(result.shape)\n",
    "#result=\n",
    "#result.sort_values(by=['id'], inplace=True)\n",
    "print(result.head())\n",
    "dfnumpy=result.to_numpy();\n",
    "X=dfnumpy[:, 1].reshape(-1, 1)\n",
    "y=dfnumpy[:, 2].reshape(-1, 1)\n",
    "tid=dfnumpy[:, 0].reshape(-1, 1)\n",
    "#print(tid)\n",
    "#arrt=X[:,0]\n",
    "#preprocessedTweets=arrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "y4WUHe1obed5",
    "outputId": "797685ab-a276-4443-eaff-bfef3ff34b4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id                                              tweet ypredicted\n",
      "0  2707  @USER Θέλω να των δω από εδώ και εμπρός αν δεν...        NOT\n",
      "1  2251  #survivorgr Α Και 60 φορές και με διαφορετικού...        NOT\n",
      "2  9814  Και μου έλεγε η γυναίκα μου το πρωί πάρε την τ...        NOT\n",
      "3  8949                   κατω τα χερια απο τον #κυρανακης        NOT\n",
      "4  6913  @USER μην μας το παιζεις πονοψυχη,κρυφορατσιστ...        NOT\n"
     ]
    }
   ],
   "source": [
    "export_csv = result.to_csv ('Greek/PredictFilegreek.csv', index = None, header=True)\n",
    "print (result.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dl9OpZZTscJ0"
   },
   "source": [
    "\n",
    "We load the data and create the validation splits as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6vMyDTjoscJ2"
   },
   "outputs": [],
   "source": [
    "from torchtext import datasets\n",
    "from torchtext import data\n",
    "\n",
    "train_val_fields = [\n",
    "    ('id', None), # we dont need this, so no processing\n",
    "    ('tweet', TEXT), # process it as label\n",
    "    ('subtask_a', LABEL) # we dont need this, so no processing\n",
    "]\n",
    "\n",
    "train_data, valid_data = data.TabularDataset.splits(path='Greek/', \n",
    "                                            format='csv', \n",
    "                                            train='TrainFilegreek.csv', \n",
    "                                            validation='TestFilegreek.csv',\n",
    "                                            fields=train_val_fields, \n",
    "                                            skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P5WzoFOL1FzS"
   },
   "outputs": [],
   "source": [
    "\n",
    "test_val_fields = [\n",
    "    ('id', None), # we dont need this, so no processing\n",
    "    ('tweet', TEXT), # process it as label\n",
    "    ('y_predicted', LABEL) # we dont need this, so no processing\n",
    "]\n",
    "predict_data = data.TabularDataset(path='Greek/PredictFilegreek.csv', format='csv', skip_header=True,fields=test_val_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "SBYyyNyFscJ7",
    "outputId": "d76784e2-55ed-4843-9199-c133c980e92e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 6995\n",
      "Number of validation examples: 1749\n",
      "Number of prediction examples: 1544\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train_data)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data)}\")\n",
    "print(f\"Number of prediction examples: {len(predict_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "YuIRlHjTscKC",
    "outputId": "d8cf9247-2620-42a3-d8a0-ba0e72899f61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tweet': [466, 97317, 33617, 31796, 13140, 70887, 43202, 31821, 483, 29223, 14669, 35670, 45878, 27393, 465, 24767, 469, 17198, 30645, 64472, 27621, 22013, 70279, 53166, 487, 13140, 63444, 61399, 61399, 22013, 70279, 53166, 168, 28174, 12961, 38278], 'subtask_a': 'NOT'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "n2YLmUNGscKI",
    "outputId": "b0ccfb92-23f1-4552-ba97-3c1cc2f549ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['β', '##γη', '##κε', 'αν', '##α', '##κο', '##ιν', '##ωση', 'σ', '##π', '##υ', '##ρι', '##δου', '##λα', 'α', '##κα', 'ε', '##λ', '##λη', '##νι', '##δα', 'master', '##chef', '##gr', 'χ', '##α', '##χ', '##χα', '##χα', 'master', '##chef', '##gr', '_', 'xe', '##ft', '##iles']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(vars(train_data.examples[6])['tweet'])\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q9lW7T9oscKP"
   },
   "outputs": [],
   "source": [
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "NUxtDneZscKU",
    "outputId": "c622b24a-f5b7-4904-9894-f82ff86b7a1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function _default_unk_index at 0x7faa4593b730>, {'NOT': 0, 'OFF': 1})\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G0bCJlCrscKb"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    sort=False,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oPtyTx08UCum"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "prediction_iterator = data.BucketIterator(\n",
    "    predict_data,\n",
    "    batch_size = BATCH_SIZE, \n",
    "    sort=False,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UG32xkag3-f6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P7sdPGVtscKg"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wg5pniU7duZA"
   },
   "outputs": [],
   "source": [
    "config1=BertConfig.from_pretrained('bert-base-multilingual-cased',output_hidden_states=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kT5N2vZRgpyr"
   },
   "outputs": [],
   "source": [
    "bert =BertModel.from_pretrained('bert-base-multilingual-cased',config=config1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LS8KPV7WscKl"
   },
   "outputs": [],
   "source": [
    "#concatenating the hidden layers in pytorch adapted and inspired from:\n",
    "#https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/#32-understanding-the-output\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import BertConfig\n",
    "class BERTGRUSentiment(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                 n_layers,\n",
    "                 bidirectional,\n",
    "                 dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert = bert\n",
    "        \n",
    "        \n",
    "        embedding_dim = bert.config.to_dict()['hidden_size']*4\n",
    "        ########################################LSTM LAYER ADDED\n",
    "        self.rnn = nn.LSTM(embedding_dim,\n",
    "                          hidden_dim,\n",
    "                          num_layers = n_layers,\n",
    "                          bidirectional = bidirectional,\n",
    "                          batch_first = True,\n",
    "                          dropout = 0 if n_layers < 2 else dropout)\n",
    "        \n",
    "        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        xmask=[]\n",
    "        #text = [batch size, sent len]\n",
    "        for b in text:\n",
    "            attention_mask = [int(i > 0) for i in b] \n",
    "            att=torch.tensor(attention_mask)\n",
    "            xmask.append(att)\n",
    "\n",
    "        x_att_mask_pytorch=torch.stack(xmask).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embedded= self.bert(text,attention_mask=x_att_mask_pytorch)\n",
    "        hidden=embedded[-1]\n",
    "        #print(hidden[12].size())\n",
    "        #print(embedded[0].size())\n",
    "        #print(len(hidden))\n",
    "        '''\n",
    "        encoded_layers=hidden\n",
    "        print (\"Number of layers:\", len(encoded_layers))\n",
    "        layer_i = 0\n",
    "\n",
    "        print (\"Number of batches:\", len(encoded_layers[layer_i]))\n",
    "        batch_i = 0\n",
    "\n",
    "        print (\"Number of tokens:\", len(encoded_layers[layer_i][batch_i]))\n",
    "        token_i = 0\n",
    "\n",
    "        print (\"Number of hidden units:\", len(encoded_layers[layer_i][batch_i][token_i]))\n",
    "        '''\n",
    "        #concatenating the hidden layers inspired from https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/#32-understanding-the-output\n",
    "        token_embeddings=torch.stack(hidden, dim=0)\n",
    "        #print(token_embeddings.size())\n",
    "        token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "        #print(token_embeddings.size())\n",
    "        token_embeddings = token_embeddings.permute(1,2,0,3)\n",
    "        #print(token_embeddings.size())\n",
    "        token_vecs_cat = []\n",
    "        irnn=[]\n",
    "        for b in token_embeddings:\n",
    "          token_vecs_cat = []\n",
    "          for token in b:\n",
    "           cat_vec = torch.cat((token[-1],token[-2], token[-3], token[-4]), dim=0)\n",
    "           #print(\"cat_vec\",len(cat_vec))\n",
    "           token_vecs_cat.append(cat_vec)\n",
    "           #a=torch.Tensor(token_vecs_cat)\n",
    "           #print(len(token_vecs_cat))\n",
    "          a = torch.stack(token_vecs_cat)#.to(device)\n",
    "          irnn.append(a) \n",
    "        z=torch.stack(irnn)#.to(device)\n",
    "        '''print(z.size())\n",
    "        print ('Shape is: %d x %d x %d' % (len(z), len(z[0]),len(z[1])))\n",
    "        print(type(z))\n",
    "        print(type(z[0]))\n",
    "        print(type(z[1]))\n",
    "        '''\n",
    "        packed_output, (hidden, cell) = self.rnn(z)\n",
    "        \n",
    "        #hidden = [n layers * n directions, batch size, emb dim]\n",
    "        \n",
    "        if self.rnn.bidirectional:\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "                \n",
    "        #hidden = [batch size, hid dim]\n",
    "        \n",
    "        output = self.out(hidden)\n",
    "        \n",
    "        #output = [batch size, out dim]\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fEIqrIG8scKq"
   },
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 512\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.25\n",
    "\n",
    "model = BERTGRUSentiment(bert,\n",
    "                         HIDDEN_DIM,\n",
    "                         OUTPUT_DIM,\n",
    "                         N_LAYERS,\n",
    "                         BIDIRECTIONAL,\n",
    "                         DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sbyI1h18scKy",
    "outputId": "f1304264-650f-44a1-95b9-b8a935fb30eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 198,842,369 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QkYmbhy9scK3"
   },
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():                \n",
    "    if name.startswith('bert'):\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "kJIl5tYVscK8",
    "outputId": "761736ed-86c4-4787-ae71-afee28831f91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 20,988,929 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "colab_type": "code",
    "id": "c2QcdD9uscLB",
    "outputId": "8a8475c0-68a7-4ed4-f124-67cc12d4a886"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn.weight_ih_l0\n",
      "rnn.weight_hh_l0\n",
      "rnn.bias_ih_l0\n",
      "rnn.bias_hh_l0\n",
      "rnn.weight_ih_l0_reverse\n",
      "rnn.weight_hh_l0_reverse\n",
      "rnn.bias_ih_l0_reverse\n",
      "rnn.bias_hh_l0_reverse\n",
      "rnn.weight_ih_l1\n",
      "rnn.weight_hh_l1\n",
      "rnn.bias_ih_l1\n",
      "rnn.bias_hh_l1\n",
      "rnn.weight_ih_l1_reverse\n",
      "rnn.weight_hh_l1_reverse\n",
      "rnn.bias_ih_l1_reverse\n",
      "rnn.bias_hh_l1_reverse\n",
      "out.weight\n",
      "out.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():                \n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pdG-jBFescLF"
   },
   "source": [
    "## Train the Model\n",
    "\n",
    "As is standard, we define our optimizer and criterion (loss function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5HCgrge6scLH"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from transformers import AdamW\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZVXs0n9UscLJ"
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hxgQxlJXscLO"
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "i8VBV6PHHA7d",
    "outputId": "288a9a8d-5ec8-4c7f-f98f-2113451aba5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERTGRUSentiment(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (rnn): LSTM(3072, 512, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "  (out): Linear(in_features=1024, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.25, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 138,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SEU_NUOOscLS"
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "quYVUygE2u4v"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def binary_accuracy2(preds, y):\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    #print(\"Predicted\",rounded_preds)\n",
    "    #print(\"labelsFlattend\",y)\n",
    "    return f1_score(y.detach().cpu().numpy(), rounded_preds.detach().cpu().numpy(), average='macro'),acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HjVM0U8kscLW"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        #print(batch)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.tweet).squeeze(1)\n",
    "        #print(predictions.dtype)\n",
    "            \n",
    "        loss = criterion(predictions, batch.subtask_a)\n",
    "        #print( batch.subtask_a.dtype)\n",
    "            \n",
    "        acc = binary_accuracy(predictions, batch.subtask_a)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V_lToD-qgMvv"
   },
   "outputs": [],
   "source": [
    "kmax=0\n",
    "ktemp=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hHQ6v0ogWCYU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uhjbtc1cscLY"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    kmax=0\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    nb_eval_steps=0\n",
    "    eval_acc=0\n",
    "    eval_f1=0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.tweet).squeeze(1)\n",
    "            loss = criterion(predictions, batch.subtask_a)\n",
    "            tmpf1score,acc = binary_accuracy2(predictions, batch.subtask_a)\n",
    "            eval_f1 = eval_f1+tmpf1score\n",
    "            eval_acc=eval_acc+acc\n",
    "            nb_eval_steps += 1\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    ktemp=eval_f1/nb_eval_steps\n",
    "    if ktemp>kmax:\n",
    "      kmax=ktemp\n",
    "      print(\"SAVING MODEL for F1 score of \",ktemp )\n",
    "      torch.save({'state_dict': model.state_dict()},'/content/drive/My Drive/GreekData/bertlstm.pth.tar')\n",
    "    f1=eval_f1/nb_eval_steps\n",
    "    print(\"  F1 score: {0:.3f}\".format(f1))\n",
    "    print(\"  Accuracy score: {0:.3f}\".format(eval_acc/nb_eval_steps))\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator),f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YajpYfx2L-48"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "97GqDDBKpOa-",
    "outputId": "a521427a-236e-4428-a363-476b1527c869"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVING MODEL for F1 score of  0.4369634780540243\n",
      "  F1 score: 0.437\n",
      "  Accuracy score: 0.733\n",
      "Epoch: 01 | Epoch Time: 1m 52s\n",
      "\tTrain Loss: 0.568 | Train Acc: 71.83%\n",
      "\t Val. Loss: 0.530 |  Val. Acc: 73.32%\n",
      "SAVING MODEL for F1 score of  0.6843721582903686\n",
      "  F1 score: 0.684\n",
      "  Accuracy score: 0.763\n",
      "Epoch: 02 | Epoch Time: 1m 52s\n",
      "\tTrain Loss: 0.533 | Train Acc: 73.62%\n",
      "\t Val. Loss: 0.497 |  Val. Acc: 76.27%\n",
      "SAVING MODEL for F1 score of  0.7072472746133283\n",
      "  F1 score: 0.707\n",
      "  Accuracy score: 0.813\n",
      "Epoch: 03 | Epoch Time: 1m 51s\n",
      "\tTrain Loss: 0.517 | Train Acc: 75.93%\n",
      "\t Val. Loss: 0.464 |  Val. Acc: 81.30%\n",
      "SAVING MODEL for F1 score of  0.7414434065098013\n",
      "  F1 score: 0.741\n",
      "  Accuracy score: 0.820\n",
      "Epoch: 04 | Epoch Time: 1m 54s\n",
      "\tTrain Loss: 0.472 | Train Acc: 79.27%\n",
      "\t Val. Loss: 0.430 |  Val. Acc: 82.02%\n",
      "SAVING MODEL for F1 score of  0.7561274357919768\n",
      "  F1 score: 0.756\n",
      "  Accuracy score: 0.822\n",
      "Epoch: 05 | Epoch Time: 1m 52s\n",
      "\tTrain Loss: 0.457 | Train Acc: 80.26%\n",
      "\t Val. Loss: 0.415 |  Val. Acc: 82.24%\n",
      "SAVING MODEL for F1 score of  0.7363237221342641\n",
      "  F1 score: 0.736\n",
      "  Accuracy score: 0.837\n",
      "Epoch: 06 | Epoch Time: 1m 51s\n",
      "\tTrain Loss: 0.446 | Train Acc: 80.91%\n",
      "\t Val. Loss: 0.400 |  Val. Acc: 83.66%\n",
      "SAVING MODEL for F1 score of  0.7609544118705702\n",
      "  F1 score: 0.761\n",
      "  Accuracy score: 0.839\n",
      "Epoch: 07 | Epoch Time: 1m 52s\n",
      "\tTrain Loss: 0.433 | Train Acc: 81.57%\n",
      "\t Val. Loss: 0.396 |  Val. Acc: 83.92%\n",
      "SAVING MODEL for F1 score of  0.7504440572774996\n",
      "  F1 score: 0.750\n",
      "  Accuracy score: 0.835\n",
      "Epoch: 08 | Epoch Time: 1m 53s\n",
      "\tTrain Loss: 0.426 | Train Acc: 81.90%\n",
      "\t Val. Loss: 0.404 |  Val. Acc: 83.46%\n",
      "SAVING MODEL for F1 score of  0.7639345944041124\n",
      "  F1 score: 0.764\n",
      "  Accuracy score: 0.848\n",
      "Epoch: 09 | Epoch Time: 1m 52s\n",
      "\tTrain Loss: 0.423 | Train Acc: 81.71%\n",
      "\t Val. Loss: 0.390 |  Val. Acc: 84.80%\n",
      "SAVING MODEL for F1 score of  0.7692021531462406\n",
      "  F1 score: 0.769\n",
      "  Accuracy score: 0.852\n",
      "Epoch: 10 | Epoch Time: 1m 54s\n",
      "\tTrain Loss: 0.414 | Train Acc: 82.52%\n",
      "\t Val. Loss: 0.379 |  Val. Acc: 85.22%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "best_f1 = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    #print(\"STOP\")\n",
    "    valid_loss, valid_acc,f1 = evaluate(model, valid_iterator, criterion)\n",
    "        \n",
    "    end_time = time.time()    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        torch.save(model.state_dict(), '/content/drive/My Drive/GreekData/bertlstm.pth.tar')\n",
    "        print(\"SAVING MODEL after \")\n",
    "        print(\"Epoch Number \",epoch)\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167
    },
    "colab_type": "code",
    "id": "ArsAZzlfo5XP",
    "outputId": "42aeb7a8-b680-4bf5-e236-4c27ae4c3406"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-146-3e5149046401>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mF\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mGWRGN\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mPEQRJH\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mP3H\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'F' is not defined"
     ]
    }
   ],
   "source": [
    "F;GWRGN;PEQRJH[P3H]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OWE0TNSzpLqM"
   },
   "outputs": [],
   "source": [
    "yres=[]\n",
    "finalTid=[]\n",
    "trueLabels=[]\n",
    "predictedLabels=[]\n",
    "def predict(model, iterator):\n",
    "    kmax=0\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    nb_eval_steps=0\n",
    "    eval_acc=0\n",
    "    eval_f1=0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.tweet).squeeze(1)\n",
    "            trueLabels.append((batch.y_predicted).detach().cpu().numpy())\n",
    "            #finalTid.append(batch.id)\n",
    "            \n",
    "            predictedLabels.append(torch.round(torch.sigmoid(predictions)).detach().cpu().numpy())\n",
    "            tmpf1score,acc= binary_accuracy2(predictions, batch.y_predicted)\n",
    "            eval_f1 = eval_f1+tmpf1score\n",
    "            eval_acc=eval_acc+acc\n",
    "            nb_eval_steps += 1\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    print(\"  F1 score: {0:.3f}\".format(eval_f1/nb_eval_steps))\n",
    "    print(\"  Accuracy score: {0:.3f}\".format(eval_acc/nb_eval_steps))\n",
    "    return predictedLabels,trueLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4KhuQl0yscL3"
   },
   "outputs": [],
   "source": [
    "'''torch.save({'state_dict': model.state_dict()}, '/content/drive/My Drive/GreekData/bertlstm.pth.tar')\n",
    "'''\n",
    "\n",
    "checkpoint = torch.load('/content/drive/My Drive/GreekData/bertlstm.pth.tar')\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "d07vDgATvYuE",
    "outputId": "b0fc636f-55da-4141-a280-3bc5dff620c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  F1 score: 0.837\n",
      "  Accuracy score: 0.915\n"
     ]
    }
   ],
   "source": [
    "predictedLabels,trueLabels = predict(model, prediction_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "np9y7yR__yEK",
    "outputId": "5f5be94c-a237-4ce6-b865-38c30fcc6329"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 149,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictedLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M3FT5o1R1iNu"
   },
   "outputs": [],
   "source": [
    "finalTids=tid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v4dGp-Qw69LJ"
   },
   "outputs": [],
   "source": [
    "ytid=tid.flatten().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OZWpiN9NxCRG"
   },
   "outputs": [],
   "source": [
    "yans=[predictedLabels[i].flatten().tolist() for i in range(len(predictedLabels))]\n",
    "ytrue=[trueLabels[i].flatten().tolist() for i in range(len(trueLabels))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JHVhZGoCAuuF"
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "yans=list(chain.from_iterable(yans))\n",
    "\n",
    "ytrue=list(chain.from_iterable(ytrue))\n",
    "\n",
    "#ytid=list(chain.from_iterable(ytid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RZE3WBT_7ooh"
   },
   "outputs": [],
   "source": [
    "yans1=[round(x) for x in yans]\n",
    "ytrue1=[round(x) for x in ytrue]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fWpQiosL7nH2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pA3TRQI9xOpe"
   },
   "outputs": [],
   "source": [
    "yans2=[\"NOT\" if yans1[i]==0 else \"OFF\" for i in range(len(yans1))]\n",
    "ytrue2=[\"NOT\" if ytrue1[i]==0 else \"OFF\" for i in range(len(ytrue1))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TLtb99R1xWXL"
   },
   "outputs": [],
   "source": [
    "print(yans2.count(\"OFF\"))\n",
    "print(yans2.count(\"NOT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VfOyHOenxaiD"
   },
   "outputs": [],
   "source": [
    "print(ytrue2.count(\"OFF\"))\n",
    "print(ytrue2.count(\"NOT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Lk1SK7RxdU6"
   },
   "outputs": [],
   "source": [
    "#z={'tweet':sentence_predict[],'subtask_a':y_predict}\n",
    "#print(len(z))\n",
    "C = {'id': ytid,\n",
    "        'predicted': yans2,\n",
    "     'given':ytrue2\n",
    "    }\n",
    "df = pd.DataFrame(C)\n",
    "print (df[df['id'] == 2707] )\n",
    "\n",
    "export_csv = df.to_csv ('/content/drive/My Drive/GreekData/GreekAnswerBILSTMConcatGreekfirst.csv', index = None, header=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERTLSTM",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "18c46b8340ac4d25980038762886467d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_69fc71f8bd444365aaf99f78251d7279",
      "placeholder": "​",
      "style": "IPY_MODEL_be2b53d3b6364a29a860507c7ee292cf",
      "value": " 996k/996k [01:22&lt;00:00, 12.0kB/s]"
     }
    },
    "2d37d4835db54c5fa4cc0e7f638647d4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "69fc71f8bd444365aaf99f78251d7279": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8185ba43fc5e410e908733860972ba6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "be2b53d3b6364a29a860507c7ee292cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cc5e498a5e0648279cec5af0f374105c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d886bdaa66134c4294f7f4a6ebbd51c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cc5e498a5e0648279cec5af0f374105c",
      "max": 995526,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8185ba43fc5e410e908733860972ba6e",
      "value": 995526
     }
    },
    "fc9811be1f5f4952aeb6cab2e92cf5e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d886bdaa66134c4294f7f4a6ebbd51c5",
       "IPY_MODEL_18c46b8340ac4d25980038762886467d"
      ],
      "layout": "IPY_MODEL_2d37d4835db54c5fa4cc0e7f638647d4"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
